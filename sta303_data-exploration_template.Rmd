---
header-includes:
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO, CE]{Taojun Wang, 1004303461}
- \fancyfoot[CO, CE]{\thepage \ of \pageref{LastPage}}
output:
  pdf_document: default
     latex_engine: xelatex
  word_document: default
urlcolor: blue
---

```{r setup, message = FALSE, echo=FALSE}
# Students: You probably shouldn't change any of the code in this chunk.

# These are the packages you will need for this activity
packages_needed <- c("tidyverse", "googledrive", "readxl", "janitor", 
                     "lubridate", "opendatatoronto", "ggthemes")

package.check <- lapply(
  packages_needed,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
    }
  }
)

# Credit: package.check based on a helpful post from Vikram Baliga https://vbaliga.github.io/verify-that-r-packages-are-installed-and-loaded/

# Load tidyverse
library(tidyverse)
library(readxl)
library(janitor)
library(opendatatoronto)
library(ggthemes)
library(Hmisc)

# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), echo = FALSE)
```


```{r getdata, eval = FALSE, echo=FALSE}
# Students: You probably shouldn't change any of the code in this chunk BUT...

# This chunk loads the most recent data from Toronto City and the data from OpenToronto.

# You have to RUN this chunk by hand to update the data as 
#   eval is set to FALSE to limit unnecessary requsts on the site.

###################################################
# Step one: Get the COVID data from Toronto City. #
###################################################

googledrive::drive_deauth()

url1 <- "https://drive.google.com/file/d/11KF1DuN5tntugNc10ogQDzFnW05ruzLH/view"
googledrive::drive_download(url1, path="data/CityofToronto_COVID-19_Daily_Public_Reporting.xlsx", overwrite = TRUE)

url2 <- "https://drive.google.com/file/d/1jzH64LvFQ-UsDibXO0MOtvjbL2CvnV3N/view"
googledrive::drive_download(url2, path = "data/CityofToronto_COVID-19_NeighbourhoodData.xlsx", overwrite = TRUE)

# this removes the url object that we don't need anymore
rm(url1, url2)

#####################################################################
# Step two: Get the data neighbourhood data from Open Data Toronto. #
#####################################################################

nbhoods_shape_raw <- list_package_resources("neighbourhoods") %>% 
  get_resource()

saveRDS(nbhoods_shape_raw, "data/neighbourhood_shapefile.Rds")

nbhood_profile <- search_packages("Neighbourhood Profile") %>%
  list_package_resources() %>% 
  filter(name == "neighbourhood-profiles-2016-csv") %>% 
  get_resource()

saveRDS(nbhood_profile, "data/neighbourhood_profile.Rds")
```


```{r load_data, echo=FALSE}
######################################################
# Step three: Load the COVID data from Toronto City. #
######################################################

# Saving the name of the file as an object and then using the object name in the
# following code is a helpful practice. Why? If we change the name of the file 
# being used, we'll only have to change it in one place. This helps us avoid 
# 'human error'.

daily_data <- "data/CityofToronto_COVID-19_Daily_Public_Reporting2.xlsx"

# Cases reported by date
reported_raw <- read_excel(daily_data, sheet = 5) %>% 
  clean_names()

# Cases by outbreak type
outbreak_raw <- read_excel(daily_data, sheet = 3) %>% 
  clean_names()

# When was this data updated?
date_daily <- read_excel(daily_data, sheet = 1) %>% 
  clean_names()

# By neighbourhood
neighbourood_data <- "data/CityofToronto_COVID-19_NeighbourhoodData.xlsx"

# Cases reported by date
nbhood_raw <- read_excel(neighbourood_data, sheet = 2) %>% 
  clean_names()

# Date the neighbourhood data was last updated
date_nbhood <- read_excel(neighbourood_data, sheet = 1) %>% 
  clean_names()

#don't need these anymore
rm(daily_data, neighbourood_data)

#############################################################
# Step four: Load the neighbourhood data from Toronto City. #
#############################################################

# Get neighbourhood profile data
nbhood_profile <- readRDS("data/neighbourhood_profile.Rds")

# Get shape data for mapping 
nbhoods_shape_raw <- readRDS("data/neighbourhood_shapefile.Rds") %>% 
  sf::st_as_sf() ## Makes sure shape info is in the most up to date format

```

Code last run `r Sys.Date()`.  
Daily: `r date_daily[1,1]`.   
Neighbourhood: `r date_nbhood[1,1]`. 

# Task 1: Daily cases
## Data wrangling

```{r cases_dw}
reported <- reported_raw %>% mutate_if(is.numeric, ~replace(., is.na(.), 0))
reported$reported_date <- as.Date(reported$reported_date)
reported <- gather(reported, conditon, counts, -reported_date)
reported$conditon <- reported$conditon[order(match(reported$conditon,c("recovered","active","deceased")))]
reported$conditon <- capitalize(reported$conditon)
``` 

\newpage
## Data visualization

```{r cases_vis, warning=FALSE}
text <- paste("Created by: for STA303/1002, U of T \n Source: Ontario Ministry of Health, Integrated Public Health Information System and CORES \n",str_c(date_daily[1,1]))
ggplot(reported,aes(x=reported_date,y=counts))+geom_bar(aes(fill = conditon),stat="identity")+labs(title = "Cases reported by day in Toronto, Canada", subtitle = "Confirmed and probable cases", caption = text) + xlab("Reported Date")+ylab("Cases")+xlim(date("2020-01-01"),Sys.Date()) + theme(legend.title = element_blank(),legend.position = c(0.15, 0.8)) + scale_fill_manual(values=c("#003F5C","#B9CA5D", "#86BCB6"))

```
\newpage
# Task 2: Outbreak type
## Data wrangling


```{r outbreak_dw}
outbreak <-outbreak_raw
outbreak$episode_week <- date(outbreak$episode_week)
outbreak$outbreak_or_sporadic <- str_replace_all(outbreak$outbreak_or_sporadic,"OB Associated","Outbreak Assoicated")
outbreak2 <- spread(outbreak, outbreak_or_sporadic,cases)
outbreak2 <- outbreak2 %>% mutate_if(is.numeric, ~replace(., is.na(.), 0))
outbreak3 <- gather(outbreak2, outbreak_or_sporadic,cases, -episode_week)
outbreak$outbreak_or_sporadic <- outbreak$outbreak_or_sporadic[order(match(outbreak$outbreak_or_sporadic,c("Sporadic", "Outbreak Associated")))]
outbreak <- outbreak %>% group_by(episode_week) %>% mutate(total_cases = sum(cases))
outbreak$outbreak_or_sporadic <-factor(outbreak$outbreak_or_sporadic,levels=c("Sporadic","Outbreak Assoicated"))
outbreak3$outbreak_or_sporadic <-factor(outbreak3$outbreak_or_sporadic,levels=c("Sporadic","Outbreak Assoicated"))
```

\newpage
## Data visualization

```{r outbreak_vis, warning=FALSE}
ggplot(outbreak3,aes(x=episode_week,y=cases,fill = outbreak_or_sporadic))+geom_bar(stat = "identity") + labs(title = "Cases by outbreak type and week in Toronto, Canada", subtitle = "Confirmed and probable cases", caption = text) + xlab("Date")+ylab("Cases count")+xlim(date("2020-01-01"),Sys.Date()+7) + theme(legend.title = element_blank(),legend.position = c(0.15, 0.8)) + scale_fill_manual(values=c("#86BCB6", "#B9CA5D"))+scale_x_date(labels = scales::date_format("%d %b %y"), limits = as.Date(c("2020-01-01","2021-02-01")))
#factor(outbreak_or_sporadic,levels=c("Sporadic","Outbreak Assoicated"))
```

\newpage
# Task 3: Neighbourhoods
## Data wrangling: part 1

```{r nbhood_dw_1}
income <- nbhood_profile %>% filter(Category=="Income") %>%  filter(Topic=="Low income in 2015") %>% filter(Characteristic=="  18 to 64 years (%)") %>% filter(`_id` == 1143)
income <- gather(income, neighbourhood_name, percentage, -c( `_id`, Category,Characteristic, Topic, `Data Source`, ))
income$percentage <- parse_number(income$percentage)

```

## Data wrangling: part 2
  
```{r nbhood_dw_2}
nbhoods_all <- nbhoods_shape_raw %>% mutate(neighbourhood_name = gsub("\\s*\\([^\\)]+\\)","",AREA_NAME))
nbhoods_all <- nbhoods_all[order(nbhoods_all$neighbourhood_name),]
nbhood_raw <- nbhood_raw[order(nbhood_raw$neighbourhood_name),]
nbhoods_all$neighbourhood_name <-str_replace_all(nbhoods_all$neighbourhood_name,"Cabbagetown-South St.James Town","Cabbagetown-South St. James Town")
nbhood_raw <- nbhood_raw[!(nbhood_raw$neighbourhood_name=="Missing Address/Postal Code"),]
nbhoods_all$neighbourhood_name <- str_replace_all(nbhoods_all$neighbourhood_name,"Mimico","Mimico (includes Humber Bay Shores)")
nbhoods_all$neighbourhood_name <- str_replace_all(nbhoods_all$neighbourhood_name,"North St.James Town","North St. James Town")
nbhoods_all$neighbourhood_name <- str_replace_all(nbhoods_all$neighbourhood_name,"Weston-Pellham Park","Weston-Pelham Park")
nbhoods_all <-merge(nbhoods_all,nbhood_raw)
low_income <- income[,-c(1,2,3,4,5)]
low_income <- low_income[!(low_income$neighbourhood_name == "City of Toronto"),]
nbhoods_all <- merge(nbhoods_all,low_income)
names(nbhoods_all)[names(nbhoods_all) == 'rate_per_100_000_people'] <- 'rate_per_100_000'
```

## Data wrangling: part 3

```{r nbhood_dw_3}
nbhoods_final <- nbhoods_all %>% mutate(med_income = median(percentage), med_rate = median(rate_per_100_000)) %>% mutate(nbhood_type = ifelse(percentage >= 16.4, yes = ifelse(rate_per_100_000 >= 2609.757, yes = "Higher low income rate, higher case rate", no = "Higher low income rate, lower case rate”" ), no = ifelse(rate_per_100_000 >= 2609.757, yes = "Lower low income rate, higher case rate", no = "Lower low income rate, Lower case rate") ))

```

\newpage
## Data visualization

```{r neighbourhood_graphs_1, fig.height=4}
ggplot(data = nbhoods_final) + geom_sf(aes(fill=percentage)) + theme_map() + scale_fill_gradient(name= "% low income", low = "darkgreen", high = "lightgrey") +  labs(title = "Percentage of 18 to 64 year olds living in a low income family (2015)", subtitle = "Neighbourhoods of Toronto, Canada", caption = text) + theme(legend.position = c(1,0.15))
``` 

\newpage

```{r neighbourhood_graphs_2, fig.height=4}
ggplot(data = nbhoods_final) + geom_sf(aes(fill=rate_per_100_000)) + theme_map() + scale_fill_gradient(name= "Cases per 100,000 people", low = "white", high = "DarkOrange") +  labs(title = "COVID−19 cases per 100,000, by neighbourhood in Toronto, Canada", caption = text) + theme(legend.position = c(1,0.15))
```

\newpage

```{r neighbourhood_graphs_3, fig.height=4}
text2 <- paste("Created by: Taojun Wang for STA303/1002, U of T \n Income data source: Census Profile 98−316−X2016001 via OpenData Toronto \n COVID data source: Ontario Ministry of Health, Integrated Public \n Health Information System and CORES \n",str_c(date_daily[1,1]))
ggplot(data = nbhoods_final) + geom_sf(aes(fill=nbhood_type)) + theme_map() + labs(title = "COVID−19 cases per 100,000, by neighbourhood in Toronto, Canada", caption = text2) + theme(legend.position = c(0.75,-0.035))+guides(fill=guide_legend(title="% of 18 to 64 year−olds in
low income families and
COVID−19 case rates"))
```




```{r, eval = FALSE}
# This chunk of code helps you prepare your assessment for submission on Crowdmark
# This is optional. If it isn't working, you can do it manually/take another approach.

# Run this chunk by hand after knitting your final version of your pdf for submission.
# A new file called 'to_submit' will appear in your working directory with each page of your assignment as a separate pdf.

# Install the required packages
if(!match("staplr", installed.packages()[,1], nomatch = FALSE))
  {install.packages("staplr")}

# Don't edit anything in this function
prep_for_crowdmark <- function(pdf=NULL){
  # Get the name of the file you're currently in. 
  this_file <- rstudioapi::getSourceEditorContext()$path
  pdf_name <- sub(".Rmd", ".pdf", sub('.*/', '', this_file))
  
  # Create a file called to_submit to put the individual files in
  # This will be in the same folder as this file is saved
  if(!match("to_submit", list.files(), nomatch = FALSE))
    {dir.create("to_submit")}
 
  # Split the files
  if(is.null(pdf)){
  staplr::split_pdf(pdf_name, output_directory = "to_submit", prefix = "page_")} else {
    staplr::split_pdf(pdf, output_directory = "to_submit", prefix = "page_") 
  }
}

prep_for_crowdmark()

```
```